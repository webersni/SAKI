{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179b0108-5347-4315-b4c3-d2a3c3c432fc",
   "metadata": {},
   "source": [
    "# SAKI Exercise 4: Smart Factory\n",
    "\n",
    "Import relevant libraries and set warehouse properties and encodings for colors and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0ab046-d56d-48c4-aa80-e3ecc7863e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "color_to_index = {\n",
    "    \"red\": 0,\n",
    "    \"blue\": 1,\n",
    "    \"white\": 2,\n",
    "    \"empty\": 3\n",
    "}\n",
    "\n",
    "index_to_color = [\"red\", \"blue\", \"white\", \"empty\"]\n",
    "\n",
    "actions_to_index = {\n",
    "    \"store\": 0,\n",
    "    \"restore\": 1\n",
    "}\n",
    "\n",
    "index_to_action = [\"store\", \"restore\"]\n",
    "\n",
    "probability_distribution = {\n",
    "    \"red\": 1/3,\n",
    "    \"blue\": 1/3,\n",
    "    \"white\": 1/3\n",
    "}\n",
    "\n",
    "# Assert probabilities sum up to 1\n",
    "assert round(sum(probability_distribution.values())) == 1, \"Probabilities of colors must sum up to 1!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19288bf2-fac1-4c36-b5fc-d82fc90d7387",
   "metadata": {},
   "source": [
    "Define a class for a virtual representation of a warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a76dad-70ef-4816-9396-8974677b1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Warehouse(object):\n",
    "    \"\"\"\n",
    "    Represents warehouse and handles optimizing.\n",
    "    \"\"\"\n",
    "    def __init__(self, layout, possible_fillings, actions, possible_next_colors):\n",
    "        \"\"\"\n",
    "        Constructor method for warehouse class.\n",
    "        \"\"\"\n",
    "        self.layout = layout\n",
    "        self.possible_fillings = possible_fillings\n",
    "        self.actions = actions\n",
    "        self.possible_next_colors = possible_next_colors\n",
    "        \n",
    "        self.state_matrix = self.generate_state_matrix()\n",
    "        self.transition_matrix = None\n",
    "        self.reward_matrix = None\n",
    "        self.policy = None\n",
    "        \n",
    "        # Hyperparameter\n",
    "        self.reward_parameter = 10\n",
    "        \n",
    "    def generate_state_matrix(self):\n",
    "        \"\"\"\n",
    "        Generates a state matrix for the warehouse properties given.\n",
    "        \"\"\"\n",
    "\n",
    "        # Multiply warehouse rows and columns to get number of fields\n",
    "        fields = self.layout[0] * self.layout[1]\n",
    "\n",
    "        # Get ranges for warehouse properties\n",
    "        possible_fillings = range(self.possible_fillings)\n",
    "        possible_actions = range(self.actions)\n",
    "        possible_next_colors = range(self.possible_next_colors)\n",
    "\n",
    "        # Get cartesian product of fields and action and next_color\n",
    "        states_product = itertools.product(itertools.product(possible_fillings, repeat=fields),\n",
    "                                           itertools.product(possible_actions, possible_next_colors))\n",
    "\n",
    "        # Put everything together and return state matrix as numpy array\n",
    "        return np.array([field_state + action_color_state for field_state, action_color_state in states_product])\n",
    "    \n",
    "    def generate_transition_matrix(self):\n",
    "        \"\"\"\n",
    "        Generate transition probability matrix for the states and the warehouse layout given.\n",
    "        \"\"\"\n",
    "\n",
    "        def normalize(field):\n",
    "            \"\"\"\n",
    "            Helper function to normalize rows to make sum of probabilities = 1\n",
    "            \"\"\"\n",
    "\n",
    "            row_sum = np.sum(transition_matrix[field, state_1, :])\n",
    "\n",
    "            if row_sum > 0:\n",
    "                transition_matrix[field, state_1, :] /= row_sum\n",
    "            # If there is no 1, we need to make the diagonal value to 1 to create a stochastic matrix in the end\n",
    "            else:\n",
    "                transition_matrix[field, state_1, state_1] = 1\n",
    "\n",
    "        # Multiply warehouse rows and columns to get number of fields\n",
    "        fields = self.layout[0] * self.layout[1]\n",
    "\n",
    "        # Get number of states\n",
    "        number_states = self.state_matrix.shape[0]\n",
    "\n",
    "        # Initialize transition matrix with 0s\n",
    "        # Get fields + 1 as shape for 3rd dimension to have start/stop position outside of warehouse\n",
    "        transition_matrix = np.zeros((fields + 1, \n",
    "                                      number_states,  # Matrix should have shape states x states\n",
    "                                      number_states),\n",
    "                                    dtype=np.float16)\n",
    "\n",
    "        # Write to transition matrix\n",
    "        for state_1 in range(number_states):\n",
    "            for field in range(fields):\n",
    "                for state_2 in range(number_states):\n",
    "                    state_1_array = self.state_matrix[state_1]\n",
    "                    state_2_array = self.state_matrix[state_2]\n",
    "\n",
    "                    if index_to_action[state_1_array[fields]] == \"store\":\n",
    "                        # If field in state_1 is empty and next color of state_1 and color in field of state_2 match\n",
    "                        if state_1_array[field] == color_to_index[\"empty\"] and state_2_array[field] == state_1_array[fields + 1]:\n",
    "                            # Make transition possible\n",
    "                            color_prob = probability_distribution[index_to_color[state_1_array[fields + 1]]]\n",
    "                            transition_matrix[field, state_1, state_2] = color_prob\n",
    "                        else:\n",
    "                            # If it's not possible, stay in start/stop position\n",
    "                            transition_matrix[fields, state_1, state_2] = 1\n",
    "                    else: # Restore instruction\n",
    "                        # If color in field in state_1 is the next to restore and this field is empty in state_2\n",
    "                        if state_1_array[field] == state_1_array[fields + 1] and state_2_array[field] == color_to_index[\"empty\"]:\n",
    "                            # Make transition possible\n",
    "                            color_prob = probability_distribution[index_to_color[state_1_array[fields + 1]]]\n",
    "                            transition_matrix[field, state_1, state_2] = color_prob\n",
    "                        else:\n",
    "                            # If it's not possible, stay in start/stop position\n",
    "                            transition_matrix[fields, state_1, state_2] = 1\n",
    "\n",
    "                # Normalize row for field\n",
    "                normalize(field)\n",
    "\n",
    "            # Normalize start/stop position\n",
    "            # Depends on state_1\n",
    "            normalize(fields)\n",
    "        \n",
    "        self.transition_matrix = transition_matrix\n",
    "        \n",
    "        return transition_matrix\n",
    "    \n",
    "    def generate_reward_matrix(self, transition_matrix=None):\n",
    "        \"\"\"\n",
    "        Generate reward matrix for the transition matrix and the warehouse layout given.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.transition_matrix is None and transition_matrix is None:\n",
    "            raise ValueError(\"No transition matrix found. Please run generate_transition_matrix() or pass one.\")\n",
    "        elif self.transition_matrix is not None:\n",
    "            transition_matrix = self.transition_matrix\n",
    "\n",
    "        # Get Manhattan distance of warehouse\n",
    "        distances = np.ones(self.layout)\n",
    "        distances += np.arange(self.layout[1])\n",
    "        distances = (distances.T + np.arange(self.layout[0])).T\n",
    "\n",
    "        distances = distances.flatten()\n",
    "\n",
    "        # Add distance for start/stop position\n",
    "        # This parameter can be tuned\n",
    "        distances = np.append(distances, 7)\n",
    "        \n",
    "        # Iterate through fields-dimension of transition_matrix\n",
    "        # Use list because appending to a list is more efficient\n",
    "        rewards = []\n",
    "        for field_index, states_matrix in enumerate(transition_matrix):\n",
    "            # Init with 0s to account for actions without reward\n",
    "            rewards_for_field = np.zeros(states_matrix.shape, dtype=np.float16)\n",
    "            reward = self.reward_parameter - distances[field_index]\n",
    "            # Write reward to each index where probability is higher than 0 but less than 1\n",
    "            rewards_for_field[np.where(np.logical_and(states_matrix > 0, states_matrix < 1))] = reward\n",
    "            \n",
    "            # Give more reward for moving less if transition is more likely than expected and vice versa\n",
    "            # This parameters can be tuned\n",
    "            if field_index == 0:\n",
    "                rewards_for_field[np.where(np.logical_and(states_matrix > 0, states_matrix >= 1/3, states_matrix < 1))] += 2\n",
    "                rewards_for_field[np.where(np.logical_and(states_matrix > 0, states_matrix < 1/3, states_matrix < 1))] -= 1\n",
    "            elif field_index == 1 or field_index == 2:\n",
    "                rewards_for_field[np.where(np.logical_and(states_matrix > 0, states_matrix >= 1/3, states_matrix < 1))] -= 1\n",
    "                rewards_for_field[np.where(np.logical_and(states_matrix > 0, states_matrix < 1/3, states_matrix < 1))] += 1             \n",
    "            elif field_index == 3:\n",
    "                rewards_for_field[np.where(np.logical_and(states_matrix > 0, states_matrix >= 1/3, states_matrix < 1))] -= 2\n",
    "                rewards_for_field[np.where(np.logical_and(states_matrix > 0, states_matrix < 1/3, states_matrix < 1))] += 2\n",
    "                \n",
    "            rewards.append(rewards_for_field)\n",
    "\n",
    "        rewards = np.array(rewards)\n",
    "        self.reward_matrix = rewards\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def get_optimal_policy(self, discount, verbose=False):\n",
    "        \"\"\"\n",
    "        Run Value Iteration to find optimal policy. \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.transition_matrix is None:\n",
    "            self.generate_transition_matrix()\n",
    "        if self.reward_matrix is None:\n",
    "            self.generate_reward_matrix()\n",
    "        \n",
    "        vi = mdptoolbox.mdp.ValueIteration(transitions=self.transition_matrix, reward=self.reward_matrix, discount=discount)\n",
    "        \n",
    "        if verbose:\n",
    "            vi.setVerbose()\n",
    "        \n",
    "        vi.run()\n",
    "        \n",
    "        self.policy = vi.policy\n",
    "        \n",
    "        return vi.policy\n",
    "    \n",
    "    def evaluate(self, test_file):\n",
    "        \"\"\"\n",
    "        Run evaluation of optimal policy against a greedy strategy. Only works for a 2x2 matrix for now.\n",
    "        \"\"\"\n",
    "        \n",
    "        def get_next_shelf(state):\n",
    "            \"\"\"\n",
    "            Helper function for greedy strategy.\n",
    "            \"\"\"\n",
    "            filling = 3 if state[4] == 0 else state[5]\n",
    "            for i in range(len(state) - 2):\n",
    "                if state[i] == filling:\n",
    "                    return i\n",
    "            return 4\n",
    "                \n",
    "        # Set distances for evaluating\n",
    "        distances = np.array([1,2,2,3,0])\n",
    "        \n",
    "        # Read test file with instructions\n",
    "        test_instructions = []\n",
    "        with open(test_file, \"r\") as file:\n",
    "            for line in file:\n",
    "                action, color = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "                test_instructions.append((actions_to_index[action], color_to_index[color]))\n",
    "        \n",
    "        # Init counter and states with empty shelves\n",
    "        distance_moved_mdp = 0\n",
    "        distance_moved_greedy = 0\n",
    "        state_mdp = np.array((3,) * self.layout[0] * self.layout[1] + (0, 0))\n",
    "        state_greedy = np.array((3,) * self.layout[0] * self.layout[1] + (0, 0))\n",
    "        \n",
    "        # Follow test instructions\n",
    "        for instr in test_instructions:\n",
    "            state_mdp[4] = instr[0] # Set instruction\n",
    "            state_mdp[5] = instr[1] # Set color of item\n",
    "            state_greedy[4] = instr[0] # Set instruction\n",
    "            state_greedy[5] = instr[1] # Set color of item\n",
    "            \n",
    "            # Get move from policy\n",
    "            next_move_mdp = self.policy[np.where((self.state_matrix == state_mdp).all(axis=1))[0][0]]\n",
    "            # Get move for greedy\n",
    "            next_move_greedy = get_next_shelf(state_greedy)\n",
    "            if instr[0] == 0: # store\n",
    "                state_mdp[next_move_mdp] = instr[1] if next_move_mdp != 4 else state_mdp[next_move_mdp]\n",
    "                state_greedy[next_move_greedy] = instr[1] if next_move_greedy != 4 else state_greedy[next_move_greedy]\n",
    "            else: # restore\n",
    "                state_mdp[next_move_mdp] = color_to_index[\"empty\"] if next_move_mdp != 4 else state_mdp[next_move_mdp]\n",
    "                state_greedy[next_move_greedy] = color_to_index[\"empty\"] if next_move_greedy != 4 else state_greedy[next_move_greedy]\n",
    "            \n",
    "            # Add up distance moved\n",
    "            distance_moved_mdp += distances[next_move_mdp]\n",
    "            distance_moved_greedy += distances[next_move_greedy]\n",
    "\n",
    "        print(\"Total Manhattan distance moved using optimal policy: \" + str(distance_moved_mdp))\n",
    "        print(\"Total Manhattan distance moved using greedy strategy: \" + str(distance_moved_greedy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56db959-21bc-4ce6-9e4a-1ecfd23da571",
   "metadata": {},
   "source": [
    "Initialize warehouse with properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3038d2a7-91dc-48ac-9198-e3b1d6ffd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_warehouse = Warehouse(layout=(2,2), possible_fillings=4, actions=2, possible_next_colors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca5257-38d4-483f-bc93-1bc66e082ab7",
   "metadata": {},
   "source": [
    "Get transition matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd094ad-c2c7-43b0-894b-33dbc9270840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 1.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 1.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        ...,\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 1.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 1.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         1.00e+00]],\n",
       "\n",
       "       [[1.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 1.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 1.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        ...,\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 1.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 1.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         1.00e+00]],\n",
       "\n",
       "       [[1.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 1.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 1.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        ...,\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 1.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 1.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         1.00e+00]],\n",
       "\n",
       "       [[1.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 1.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 1.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        ...,\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 1.00e+00, 0.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 1.00e+00,\n",
       "         0.00e+00],\n",
       "        [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00,\n",
       "         1.00e+00]],\n",
       "\n",
       "       [[6.51e-04, 6.51e-04, 6.51e-04, ..., 6.51e-04, 6.51e-04,\n",
       "         6.51e-04],\n",
       "        [6.51e-04, 6.51e-04, 6.51e-04, ..., 6.51e-04, 6.51e-04,\n",
       "         6.51e-04],\n",
       "        [6.51e-04, 6.51e-04, 6.51e-04, ..., 6.51e-04, 6.51e-04,\n",
       "         6.51e-04],\n",
       "        ...,\n",
       "        [6.51e-04, 6.51e-04, 6.51e-04, ..., 6.51e-04, 6.51e-04,\n",
       "         6.51e-04],\n",
       "        [6.51e-04, 6.51e-04, 6.51e-04, ..., 6.51e-04, 6.51e-04,\n",
       "         6.51e-04],\n",
       "        [6.51e-04, 6.51e-04, 6.51e-04, ..., 6.51e-04, 6.51e-04,\n",
       "         6.51e-04]]], dtype=float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_warehouse.generate_transition_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab43f10-0ef3-4472-812b-e21190dd47fb",
   "metadata": {},
   "source": [
    "Get reward matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12fd405d-8cff-42b5-8a0f-275980a1f1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  2.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ...,  2.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  2.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  2.]],\n",
       "\n",
       "       [[-1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., -1.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0., -1., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., -1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0., -1.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0., -1.]],\n",
       "\n",
       "       [[-1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., -1.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0., -1., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., -1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0., -1.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0., -1.]],\n",
       "\n",
       "       [[-2.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., -2.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0., -2., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., -2.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0., -2.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0., -2.]],\n",
       "\n",
       "       [[ 3.,  3.,  3., ...,  3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3., ...,  3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3., ...,  3.,  3.,  3.],\n",
       "        ...,\n",
       "        [ 3.,  3.,  3., ...,  3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3., ...,  3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3., ...,  3.,  3.,  3.]]], dtype=float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_warehouse.generate_reward_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03cb0e7-e21f-4b5f-aac8-bb7c3b227678",
   "metadata": {},
   "source": [
    "Find optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916d5f5f-85a8-4555-8b41-bba64f5f4628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tV-variation\n",
      "    1\t\t  6.0\n",
      "    2\t\t  0.8013668060302734\n",
      "    3\t\t  0.08138541736752813\n",
      "    4\t\t  0.008398637719601254\n",
      "    5\t\t  0.0008824624667838066\n",
      "Iterating stopped, epsilon-optimal policy found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(my_warehouse.get_optimal_policy(discount=0.9, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb62ff4-bf58-4d51-a47e-5ea88388c756",
   "metadata": {},
   "source": [
    "Evaluate it against a greedy strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1097b105-a974-4af2-be49-d2aff6976d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Manhattan distance moved using optimal policy: 135\n",
      "Total Manhattan distance moved using greedy strategy: 114\n"
     ]
    }
   ],
   "source": [
    "my_warehouse.evaluate(\"test.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
